{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.765625,
  "eval_steps": 500,
  "global_step": 10000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.048828125,
      "grad_norm": 23.9404354095459,
      "learning_rate": 1.9902343750000002e-05,
      "loss": 90.8783,
      "step": 50
    },
    {
      "epoch": 0.09765625,
      "grad_norm": 17.42243003845215,
      "learning_rate": 1.9804687500000003e-05,
      "loss": 146.0439,
      "step": 100
    },
    {
      "epoch": 0.146484375,
      "grad_norm": 12.863730430603027,
      "learning_rate": 1.970703125e-05,
      "loss": 180.6598,
      "step": 150
    },
    {
      "epoch": 0.1953125,
      "grad_norm": 9.513699531555176,
      "learning_rate": 1.9609375e-05,
      "loss": 196.6178,
      "step": 200
    },
    {
      "epoch": 0.244140625,
      "grad_norm": 7.017597198486328,
      "learning_rate": 1.951171875e-05,
      "loss": 197.3808,
      "step": 250
    },
    {
      "epoch": 0.29296875,
      "grad_norm": 5.161116600036621,
      "learning_rate": 1.9414062500000002e-05,
      "loss": 186.8364,
      "step": 300
    },
    {
      "epoch": 0.341796875,
      "grad_norm": 3.7133285999298096,
      "learning_rate": 1.9316406250000002e-05,
      "loss": 169.0487,
      "step": 350
    },
    {
      "epoch": 0.390625,
      "grad_norm": 2.5334599018096924,
      "learning_rate": 1.9218750000000003e-05,
      "loss": 146.7145,
      "step": 400
    },
    {
      "epoch": 0.439453125,
      "grad_norm": 1.651615023612976,
      "learning_rate": 1.912109375e-05,
      "loss": 122.81,
      "step": 450
    },
    {
      "epoch": 0.48828125,
      "grad_norm": 1.0441550016403198,
      "learning_rate": 1.90234375e-05,
      "loss": 99.227,
      "step": 500
    },
    {
      "epoch": 0.537109375,
      "grad_norm": 0.6318907737731934,
      "learning_rate": 1.892578125e-05,
      "loss": 78.7815,
      "step": 550
    },
    {
      "epoch": 0.5859375,
      "grad_norm": 0.44630441069602966,
      "learning_rate": 1.8828125000000002e-05,
      "loss": 65.9259,
      "step": 600
    },
    {
      "epoch": 0.634765625,
      "grad_norm": 0.34302961826324463,
      "learning_rate": 1.8730468750000003e-05,
      "loss": 59.6378,
      "step": 650
    },
    {
      "epoch": 0.68359375,
      "grad_norm": 0.27531516551971436,
      "learning_rate": 1.8632812500000003e-05,
      "loss": 55.3945,
      "step": 700
    },
    {
      "epoch": 0.732421875,
      "grad_norm": 0.2144961804151535,
      "learning_rate": 1.853515625e-05,
      "loss": 53.0966,
      "step": 750
    },
    {
      "epoch": 0.78125,
      "grad_norm": 0.17156414687633514,
      "learning_rate": 1.84375e-05,
      "loss": 51.5468,
      "step": 800
    },
    {
      "epoch": 0.830078125,
      "grad_norm": 0.1485338658094406,
      "learning_rate": 1.833984375e-05,
      "loss": 50.8261,
      "step": 850
    },
    {
      "epoch": 0.87890625,
      "grad_norm": 0.1321549266576767,
      "learning_rate": 1.8242187500000002e-05,
      "loss": 50.7436,
      "step": 900
    },
    {
      "epoch": 0.927734375,
      "grad_norm": 0.11656296253204346,
      "learning_rate": 1.8144531250000003e-05,
      "loss": 50.55,
      "step": 950
    },
    {
      "epoch": 0.9765625,
      "grad_norm": 0.11101242899894714,
      "learning_rate": 1.8046875e-05,
      "loss": 51.1262,
      "step": 1000
    },
    {
      "epoch": 1.025390625,
      "grad_norm": 0.09416352212429047,
      "learning_rate": 1.794921875e-05,
      "loss": 51.4885,
      "step": 1050
    },
    {
      "epoch": 1.07421875,
      "grad_norm": 0.09063208103179932,
      "learning_rate": 1.78515625e-05,
      "loss": 52.3396,
      "step": 1100
    },
    {
      "epoch": 1.123046875,
      "grad_norm": 0.08873464167118073,
      "learning_rate": 1.775390625e-05,
      "loss": 53.3861,
      "step": 1150
    },
    {
      "epoch": 1.171875,
      "grad_norm": 0.07775665074586868,
      "learning_rate": 1.7656250000000002e-05,
      "loss": 54.4705,
      "step": 1200
    },
    {
      "epoch": 1.220703125,
      "grad_norm": 0.0705363005399704,
      "learning_rate": 1.7558593750000003e-05,
      "loss": 55.4318,
      "step": 1250
    },
    {
      "epoch": 1.26953125,
      "grad_norm": 0.06823254376649857,
      "learning_rate": 1.74609375e-05,
      "loss": 56.544,
      "step": 1300
    },
    {
      "epoch": 1.318359375,
      "grad_norm": 0.06704414635896683,
      "learning_rate": 1.736328125e-05,
      "loss": 57.7187,
      "step": 1350
    },
    {
      "epoch": 1.3671875,
      "grad_norm": 0.06129976361989975,
      "learning_rate": 1.7265625e-05,
      "loss": 59.1745,
      "step": 1400
    },
    {
      "epoch": 1.416015625,
      "grad_norm": 0.05978723615407944,
      "learning_rate": 1.7167968750000002e-05,
      "loss": 60.7716,
      "step": 1450
    },
    {
      "epoch": 1.46484375,
      "grad_norm": 0.05642513930797577,
      "learning_rate": 1.7070312500000002e-05,
      "loss": 62.2514,
      "step": 1500
    },
    {
      "epoch": 1.513671875,
      "grad_norm": 0.057458557188510895,
      "learning_rate": 1.6972656250000003e-05,
      "loss": 63.5916,
      "step": 1550
    },
    {
      "epoch": 1.5625,
      "grad_norm": 0.05924244225025177,
      "learning_rate": 1.6875e-05,
      "loss": 65.4608,
      "step": 1600
    },
    {
      "epoch": 1.611328125,
      "grad_norm": 0.05869285762310028,
      "learning_rate": 1.677734375e-05,
      "loss": 66.7084,
      "step": 1650
    },
    {
      "epoch": 1.66015625,
      "grad_norm": 0.05827796831727028,
      "learning_rate": 1.66796875e-05,
      "loss": 68.3567,
      "step": 1700
    },
    {
      "epoch": 1.708984375,
      "grad_norm": 0.05463333800435066,
      "learning_rate": 1.6582031250000002e-05,
      "loss": 70.0038,
      "step": 1750
    },
    {
      "epoch": 1.7578125,
      "grad_norm": 0.057853832840919495,
      "learning_rate": 1.6484375000000003e-05,
      "loss": 71.5033,
      "step": 1800
    },
    {
      "epoch": 1.806640625,
      "grad_norm": 0.05490129068493843,
      "learning_rate": 1.638671875e-05,
      "loss": 73.1439,
      "step": 1850
    },
    {
      "epoch": 1.85546875,
      "grad_norm": 0.0508166179060936,
      "learning_rate": 1.62890625e-05,
      "loss": 74.9465,
      "step": 1900
    },
    {
      "epoch": 1.904296875,
      "grad_norm": 0.051604319363832474,
      "learning_rate": 1.619140625e-05,
      "loss": 76.7517,
      "step": 1950
    },
    {
      "epoch": 1.953125,
      "grad_norm": 0.04752565547823906,
      "learning_rate": 1.609375e-05,
      "loss": 77.916,
      "step": 2000
    },
    {
      "epoch": 2.001953125,
      "grad_norm": 0.05082530155777931,
      "learning_rate": 1.5996093750000002e-05,
      "loss": 79.9264,
      "step": 2050
    },
    {
      "epoch": 2.05078125,
      "grad_norm": 0.04871072247624397,
      "learning_rate": 1.5898437500000003e-05,
      "loss": 80.8158,
      "step": 2100
    },
    {
      "epoch": 2.099609375,
      "grad_norm": 0.04605643078684807,
      "learning_rate": 1.580078125e-05,
      "loss": 80.7276,
      "step": 2150
    },
    {
      "epoch": 2.1484375,
      "grad_norm": 0.046258486807346344,
      "learning_rate": 1.5703125e-05,
      "loss": 80.2896,
      "step": 2200
    },
    {
      "epoch": 2.197265625,
      "grad_norm": 0.04795742779970169,
      "learning_rate": 1.560546875e-05,
      "loss": 79.8747,
      "step": 2250
    },
    {
      "epoch": 2.24609375,
      "grad_norm": 0.045850738883018494,
      "learning_rate": 1.5507812500000002e-05,
      "loss": 80.0992,
      "step": 2300
    },
    {
      "epoch": 2.294921875,
      "grad_norm": 0.044515520334243774,
      "learning_rate": 1.5410156250000002e-05,
      "loss": 79.6324,
      "step": 2350
    },
    {
      "epoch": 2.34375,
      "grad_norm": 0.04549117758870125,
      "learning_rate": 1.5312500000000003e-05,
      "loss": 79.5176,
      "step": 2400
    },
    {
      "epoch": 2.392578125,
      "grad_norm": 0.04314717277884483,
      "learning_rate": 1.5214843750000002e-05,
      "loss": 79.237,
      "step": 2450
    },
    {
      "epoch": 2.44140625,
      "grad_norm": 0.043296292424201965,
      "learning_rate": 1.51171875e-05,
      "loss": 79.3997,
      "step": 2500
    },
    {
      "epoch": 2.490234375,
      "grad_norm": 0.04231366887688637,
      "learning_rate": 1.5019531250000001e-05,
      "loss": 79.1208,
      "step": 2550
    },
    {
      "epoch": 2.5390625,
      "grad_norm": 0.0425402857363224,
      "learning_rate": 1.4921875000000002e-05,
      "loss": 78.9719,
      "step": 2600
    },
    {
      "epoch": 2.587890625,
      "grad_norm": 0.042851485311985016,
      "learning_rate": 1.482421875e-05,
      "loss": 78.9245,
      "step": 2650
    },
    {
      "epoch": 2.63671875,
      "grad_norm": 0.042038965970277786,
      "learning_rate": 1.4726562500000001e-05,
      "loss": 78.7532,
      "step": 2700
    },
    {
      "epoch": 2.685546875,
      "grad_norm": 0.0422406904399395,
      "learning_rate": 1.4628906250000002e-05,
      "loss": 78.8212,
      "step": 2750
    },
    {
      "epoch": 2.734375,
      "grad_norm": 0.0403117798268795,
      "learning_rate": 1.453125e-05,
      "loss": 78.5122,
      "step": 2800
    },
    {
      "epoch": 2.783203125,
      "grad_norm": 0.042160119861364365,
      "learning_rate": 1.4433593750000001e-05,
      "loss": 78.377,
      "step": 2850
    },
    {
      "epoch": 2.83203125,
      "grad_norm": 0.03988831117749214,
      "learning_rate": 1.43359375e-05,
      "loss": 78.5285,
      "step": 2900
    },
    {
      "epoch": 2.880859375,
      "grad_norm": 0.039084915071725845,
      "learning_rate": 1.4238281250000001e-05,
      "loss": 78.48,
      "step": 2950
    },
    {
      "epoch": 2.9296875,
      "grad_norm": 0.04167331010103226,
      "learning_rate": 1.4140625000000002e-05,
      "loss": 78.4825,
      "step": 3000
    },
    {
      "epoch": 2.978515625,
      "grad_norm": 0.0405011810362339,
      "learning_rate": 1.404296875e-05,
      "loss": 78.4008,
      "step": 3050
    },
    {
      "epoch": 3.02734375,
      "grad_norm": 0.038743846118450165,
      "learning_rate": 1.3945312500000001e-05,
      "loss": 78.2597,
      "step": 3100
    },
    {
      "epoch": 3.076171875,
      "grad_norm": 0.03985349088907242,
      "learning_rate": 1.3847656250000002e-05,
      "loss": 78.3694,
      "step": 3150
    },
    {
      "epoch": 3.125,
      "grad_norm": 0.03712794557213783,
      "learning_rate": 1.375e-05,
      "loss": 78.236,
      "step": 3200
    },
    {
      "epoch": 3.173828125,
      "grad_norm": 0.03802729770541191,
      "learning_rate": 1.3652343750000001e-05,
      "loss": 78.4595,
      "step": 3250
    },
    {
      "epoch": 3.22265625,
      "grad_norm": 0.037377048283815384,
      "learning_rate": 1.3554687500000002e-05,
      "loss": 78.2291,
      "step": 3300
    },
    {
      "epoch": 3.271484375,
      "grad_norm": 0.03683096170425415,
      "learning_rate": 1.345703125e-05,
      "loss": 78.4225,
      "step": 3350
    },
    {
      "epoch": 3.3203125,
      "grad_norm": 0.03801031410694122,
      "learning_rate": 1.3359375000000001e-05,
      "loss": 78.2028,
      "step": 3400
    },
    {
      "epoch": 3.369140625,
      "grad_norm": 0.03763490170240402,
      "learning_rate": 1.3261718750000002e-05,
      "loss": 78.204,
      "step": 3450
    },
    {
      "epoch": 3.41796875,
      "grad_norm": 0.03808153048157692,
      "learning_rate": 1.31640625e-05,
      "loss": 78.3857,
      "step": 3500
    },
    {
      "epoch": 3.466796875,
      "grad_norm": 0.0370182991027832,
      "learning_rate": 1.3066406250000001e-05,
      "loss": 78.4088,
      "step": 3550
    },
    {
      "epoch": 3.515625,
      "grad_norm": 0.0372033528983593,
      "learning_rate": 1.2968750000000002e-05,
      "loss": 78.2866,
      "step": 3600
    },
    {
      "epoch": 3.564453125,
      "grad_norm": 0.03648574650287628,
      "learning_rate": 1.287109375e-05,
      "loss": 78.4814,
      "step": 3650
    },
    {
      "epoch": 3.61328125,
      "grad_norm": 0.03591421991586685,
      "learning_rate": 1.2773437500000001e-05,
      "loss": 78.3272,
      "step": 3700
    },
    {
      "epoch": 3.662109375,
      "grad_norm": 0.037494949996471405,
      "learning_rate": 1.267578125e-05,
      "loss": 78.541,
      "step": 3750
    },
    {
      "epoch": 3.7109375,
      "grad_norm": 0.036957621574401855,
      "learning_rate": 1.2578125e-05,
      "loss": 78.6359,
      "step": 3800
    },
    {
      "epoch": 3.759765625,
      "grad_norm": 0.03617069497704506,
      "learning_rate": 1.2480468750000001e-05,
      "loss": 78.4901,
      "step": 3850
    },
    {
      "epoch": 3.80859375,
      "grad_norm": 0.034058794379234314,
      "learning_rate": 1.23828125e-05,
      "loss": 78.7665,
      "step": 3900
    },
    {
      "epoch": 3.857421875,
      "grad_norm": 0.03567058593034744,
      "learning_rate": 1.2285156250000001e-05,
      "loss": 78.7969,
      "step": 3950
    },
    {
      "epoch": 3.90625,
      "grad_norm": 0.03396739065647125,
      "learning_rate": 1.2187500000000001e-05,
      "loss": 78.8531,
      "step": 4000
    },
    {
      "epoch": 3.955078125,
      "grad_norm": 0.035048868507146835,
      "learning_rate": 1.208984375e-05,
      "loss": 78.9207,
      "step": 4050
    },
    {
      "epoch": 4.00390625,
      "grad_norm": 0.033977482467889786,
      "learning_rate": 1.1992187500000001e-05,
      "loss": 79.0166,
      "step": 4100
    },
    {
      "epoch": 4.052734375,
      "grad_norm": 0.03823063522577286,
      "learning_rate": 1.1894531250000002e-05,
      "loss": 79.0632,
      "step": 4150
    },
    {
      "epoch": 4.1015625,
      "grad_norm": 0.03451039269566536,
      "learning_rate": 1.1796875e-05,
      "loss": 79.2595,
      "step": 4200
    },
    {
      "epoch": 4.150390625,
      "grad_norm": 0.03415432199835777,
      "learning_rate": 1.1699218750000001e-05,
      "loss": 79.5066,
      "step": 4250
    },
    {
      "epoch": 4.19921875,
      "grad_norm": 0.03231247514486313,
      "learning_rate": 1.1601562500000002e-05,
      "loss": 79.243,
      "step": 4300
    },
    {
      "epoch": 4.248046875,
      "grad_norm": 0.035479094833135605,
      "learning_rate": 1.150390625e-05,
      "loss": 79.6618,
      "step": 4350
    },
    {
      "epoch": 4.296875,
      "grad_norm": 0.03249198943376541,
      "learning_rate": 1.1406250000000001e-05,
      "loss": 79.7749,
      "step": 4400
    },
    {
      "epoch": 4.345703125,
      "grad_norm": 0.03905460610985756,
      "learning_rate": 1.1308593750000002e-05,
      "loss": 80.0498,
      "step": 4450
    },
    {
      "epoch": 4.39453125,
      "grad_norm": 0.032083090394735336,
      "learning_rate": 1.12109375e-05,
      "loss": 79.9606,
      "step": 4500
    },
    {
      "epoch": 4.443359375,
      "grad_norm": 0.03259943425655365,
      "learning_rate": 1.1113281250000001e-05,
      "loss": 80.1941,
      "step": 4550
    },
    {
      "epoch": 4.4921875,
      "grad_norm": 0.032248131930828094,
      "learning_rate": 1.1015625e-05,
      "loss": 80.1793,
      "step": 4600
    },
    {
      "epoch": 4.541015625,
      "grad_norm": 0.031987253576517105,
      "learning_rate": 1.091796875e-05,
      "loss": 80.2943,
      "step": 4650
    },
    {
      "epoch": 4.58984375,
      "grad_norm": 0.031462788581848145,
      "learning_rate": 1.0820312500000001e-05,
      "loss": 80.6526,
      "step": 4700
    },
    {
      "epoch": 4.638671875,
      "grad_norm": 0.03196270391345024,
      "learning_rate": 1.072265625e-05,
      "loss": 80.6582,
      "step": 4750
    },
    {
      "epoch": 4.6875,
      "grad_norm": 0.03303221985697746,
      "learning_rate": 1.0625e-05,
      "loss": 80.8909,
      "step": 4800
    },
    {
      "epoch": 4.736328125,
      "grad_norm": 0.03111075982451439,
      "learning_rate": 1.0527343750000001e-05,
      "loss": 81.0182,
      "step": 4850
    },
    {
      "epoch": 4.78515625,
      "grad_norm": 0.030331667512655258,
      "learning_rate": 1.04296875e-05,
      "loss": 81.1361,
      "step": 4900
    },
    {
      "epoch": 4.833984375,
      "grad_norm": 0.03532685339450836,
      "learning_rate": 1.033203125e-05,
      "loss": 81.2161,
      "step": 4950
    },
    {
      "epoch": 4.8828125,
      "grad_norm": 0.03126009553670883,
      "learning_rate": 1.0234375000000001e-05,
      "loss": 81.4444,
      "step": 5000
    },
    {
      "epoch": 4.931640625,
      "grad_norm": 0.03734666854143143,
      "learning_rate": 1.013671875e-05,
      "loss": 81.6336,
      "step": 5050
    },
    {
      "epoch": 4.98046875,
      "grad_norm": 0.030602524057030678,
      "learning_rate": 1.0039062500000001e-05,
      "loss": 81.8028,
      "step": 5100
    },
    {
      "epoch": 5.029296875,
      "grad_norm": 0.03758997470140457,
      "learning_rate": 9.941406250000002e-06,
      "loss": 81.9428,
      "step": 5150
    },
    {
      "epoch": 5.078125,
      "grad_norm": 0.03251773491501808,
      "learning_rate": 9.84375e-06,
      "loss": 82.3546,
      "step": 5200
    },
    {
      "epoch": 5.126953125,
      "grad_norm": 0.03180624172091484,
      "learning_rate": 9.746093750000001e-06,
      "loss": 82.4508,
      "step": 5250
    },
    {
      "epoch": 5.17578125,
      "grad_norm": 0.030776215717196465,
      "learning_rate": 9.648437500000002e-06,
      "loss": 82.5441,
      "step": 5300
    },
    {
      "epoch": 5.224609375,
      "grad_norm": 0.030079254880547523,
      "learning_rate": 9.55078125e-06,
      "loss": 82.7134,
      "step": 5350
    },
    {
      "epoch": 5.2734375,
      "grad_norm": 0.02955499477684498,
      "learning_rate": 9.453125000000001e-06,
      "loss": 82.8746,
      "step": 5400
    },
    {
      "epoch": 5.322265625,
      "grad_norm": 0.030718348920345306,
      "learning_rate": 9.35546875e-06,
      "loss": 83.1345,
      "step": 5450
    },
    {
      "epoch": 5.37109375,
      "grad_norm": 0.029198428615927696,
      "learning_rate": 9.2578125e-06,
      "loss": 83.3495,
      "step": 5500
    },
    {
      "epoch": 5.419921875,
      "grad_norm": 0.03332030773162842,
      "learning_rate": 9.160156250000001e-06,
      "loss": 83.6895,
      "step": 5550
    },
    {
      "epoch": 5.46875,
      "grad_norm": 0.02928282879292965,
      "learning_rate": 9.0625e-06,
      "loss": 83.6143,
      "step": 5600
    },
    {
      "epoch": 5.517578125,
      "grad_norm": 0.02856721170246601,
      "learning_rate": 8.96484375e-06,
      "loss": 83.8379,
      "step": 5650
    },
    {
      "epoch": 5.56640625,
      "grad_norm": 0.03340966999530792,
      "learning_rate": 8.867187500000001e-06,
      "loss": 84.1481,
      "step": 5700
    },
    {
      "epoch": 5.615234375,
      "grad_norm": 0.028106221929192543,
      "learning_rate": 8.76953125e-06,
      "loss": 84.3462,
      "step": 5750
    },
    {
      "epoch": 5.6640625,
      "grad_norm": 0.029557032510638237,
      "learning_rate": 8.671875e-06,
      "loss": 84.5868,
      "step": 5800
    },
    {
      "epoch": 5.712890625,
      "grad_norm": 0.028322391211986542,
      "learning_rate": 8.574218750000001e-06,
      "loss": 84.8261,
      "step": 5850
    },
    {
      "epoch": 5.76171875,
      "grad_norm": 0.029737159609794617,
      "learning_rate": 8.4765625e-06,
      "loss": 84.9584,
      "step": 5900
    },
    {
      "epoch": 5.810546875,
      "grad_norm": 0.028045447543263435,
      "learning_rate": 8.37890625e-06,
      "loss": 85.1999,
      "step": 5950
    },
    {
      "epoch": 5.859375,
      "grad_norm": 0.028595127165317535,
      "learning_rate": 8.281250000000001e-06,
      "loss": 85.3377,
      "step": 6000
    },
    {
      "epoch": 5.908203125,
      "grad_norm": 0.028568970039486885,
      "learning_rate": 8.18359375e-06,
      "loss": 85.553,
      "step": 6050
    },
    {
      "epoch": 5.95703125,
      "grad_norm": 0.02804313227534294,
      "learning_rate": 8.085937500000001e-06,
      "loss": 85.9193,
      "step": 6100
    },
    {
      "epoch": 6.005859375,
      "grad_norm": 0.029034050181508064,
      "learning_rate": 7.988281250000001e-06,
      "loss": 85.9964,
      "step": 6150
    },
    {
      "epoch": 6.0546875,
      "grad_norm": 0.028088118880987167,
      "learning_rate": 7.890625e-06,
      "loss": 86.3045,
      "step": 6200
    },
    {
      "epoch": 6.103515625,
      "grad_norm": 0.02867905981838703,
      "learning_rate": 7.792968750000001e-06,
      "loss": 86.4662,
      "step": 6250
    },
    {
      "epoch": 6.15234375,
      "grad_norm": 0.02783772349357605,
      "learning_rate": 7.6953125e-06,
      "loss": 86.6015,
      "step": 6300
    },
    {
      "epoch": 6.201171875,
      "grad_norm": 0.02915220521390438,
      "learning_rate": 7.5976562500000004e-06,
      "loss": 86.7591,
      "step": 6350
    },
    {
      "epoch": 6.25,
      "grad_norm": 0.02903001196682453,
      "learning_rate": 7.500000000000001e-06,
      "loss": 87.0201,
      "step": 6400
    },
    {
      "epoch": 6.298828125,
      "grad_norm": 0.028742682188749313,
      "learning_rate": 7.402343750000001e-06,
      "loss": 87.3313,
      "step": 6450
    },
    {
      "epoch": 6.34765625,
      "grad_norm": 0.027203617617487907,
      "learning_rate": 7.3046875000000005e-06,
      "loss": 87.4687,
      "step": 6500
    },
    {
      "epoch": 6.396484375,
      "grad_norm": 0.026978658512234688,
      "learning_rate": 7.20703125e-06,
      "loss": 87.6473,
      "step": 6550
    },
    {
      "epoch": 6.4453125,
      "grad_norm": 0.026772476732730865,
      "learning_rate": 7.109375000000001e-06,
      "loss": 87.7486,
      "step": 6600
    },
    {
      "epoch": 6.494140625,
      "grad_norm": 0.030534937977790833,
      "learning_rate": 7.011718750000001e-06,
      "loss": 87.9943,
      "step": 6650
    },
    {
      "epoch": 6.54296875,
      "grad_norm": 0.02671266533434391,
      "learning_rate": 6.9140625e-06,
      "loss": 88.2842,
      "step": 6700
    },
    {
      "epoch": 6.591796875,
      "grad_norm": 0.025174148380756378,
      "learning_rate": 6.816406250000001e-06,
      "loss": 88.5237,
      "step": 6750
    },
    {
      "epoch": 6.640625,
      "grad_norm": 0.02662084996700287,
      "learning_rate": 6.718750000000001e-06,
      "loss": 88.6946,
      "step": 6800
    },
    {
      "epoch": 6.689453125,
      "grad_norm": 0.028382794931530952,
      "learning_rate": 6.6210937500000004e-06,
      "loss": 88.8275,
      "step": 6850
    },
    {
      "epoch": 6.73828125,
      "grad_norm": 0.02599320188164711,
      "learning_rate": 6.5234375e-06,
      "loss": 89.063,
      "step": 6900
    },
    {
      "epoch": 6.787109375,
      "grad_norm": 0.030437080189585686,
      "learning_rate": 6.425781250000001e-06,
      "loss": 89.2349,
      "step": 6950
    },
    {
      "epoch": 6.8359375,
      "grad_norm": 0.026997605338692665,
      "learning_rate": 6.3281250000000005e-06,
      "loss": 89.5474,
      "step": 7000
    },
    {
      "epoch": 6.884765625,
      "grad_norm": 0.02929260954260826,
      "learning_rate": 6.23046875e-06,
      "loss": 89.6863,
      "step": 7050
    },
    {
      "epoch": 6.93359375,
      "grad_norm": 0.03763093799352646,
      "learning_rate": 6.132812500000001e-06,
      "loss": 89.9509,
      "step": 7100
    },
    {
      "epoch": 6.982421875,
      "grad_norm": 0.02428540028631687,
      "learning_rate": 6.035156250000001e-06,
      "loss": 90.1426,
      "step": 7150
    },
    {
      "epoch": 7.03125,
      "grad_norm": 0.028303971514105797,
      "learning_rate": 5.9375e-06,
      "loss": 90.2528,
      "step": 7200
    },
    {
      "epoch": 7.080078125,
      "grad_norm": 0.02683398872613907,
      "learning_rate": 5.83984375e-06,
      "loss": 90.33,
      "step": 7250
    },
    {
      "epoch": 7.12890625,
      "grad_norm": 0.02679884061217308,
      "learning_rate": 5.742187500000001e-06,
      "loss": 90.6748,
      "step": 7300
    },
    {
      "epoch": 7.177734375,
      "grad_norm": 0.025791991502046585,
      "learning_rate": 5.64453125e-06,
      "loss": 90.787,
      "step": 7350
    },
    {
      "epoch": 7.2265625,
      "grad_norm": 0.024780023843050003,
      "learning_rate": 5.546875e-06,
      "loss": 90.994,
      "step": 7400
    },
    {
      "epoch": 7.275390625,
      "grad_norm": 0.025560103356838226,
      "learning_rate": 5.449218750000001e-06,
      "loss": 91.0998,
      "step": 7450
    },
    {
      "epoch": 7.32421875,
      "grad_norm": 0.02599084936082363,
      "learning_rate": 5.3515625000000005e-06,
      "loss": 91.3051,
      "step": 7500
    },
    {
      "epoch": 7.373046875,
      "grad_norm": 0.029422899708151817,
      "learning_rate": 5.25390625e-06,
      "loss": 91.4426,
      "step": 7550
    },
    {
      "epoch": 7.421875,
      "grad_norm": 0.02520812302827835,
      "learning_rate": 5.156250000000001e-06,
      "loss": 91.6452,
      "step": 7600
    },
    {
      "epoch": 7.470703125,
      "grad_norm": 0.025731950998306274,
      "learning_rate": 5.0585937500000006e-06,
      "loss": 91.8657,
      "step": 7650
    },
    {
      "epoch": 7.51953125,
      "grad_norm": 0.02514568530023098,
      "learning_rate": 4.9609375e-06,
      "loss": 91.9425,
      "step": 7700
    },
    {
      "epoch": 7.568359375,
      "grad_norm": 0.025348549708724022,
      "learning_rate": 4.86328125e-06,
      "loss": 92.1765,
      "step": 7750
    },
    {
      "epoch": 7.6171875,
      "grad_norm": 0.02553187683224678,
      "learning_rate": 4.765625000000001e-06,
      "loss": 92.3936,
      "step": 7800
    },
    {
      "epoch": 7.666015625,
      "grad_norm": 0.025040648877620697,
      "learning_rate": 4.66796875e-06,
      "loss": 92.5041,
      "step": 7850
    },
    {
      "epoch": 7.71484375,
      "grad_norm": 0.029549995437264442,
      "learning_rate": 4.5703125e-06,
      "loss": 92.6468,
      "step": 7900
    },
    {
      "epoch": 7.763671875,
      "grad_norm": 0.027698686346411705,
      "learning_rate": 4.472656250000001e-06,
      "loss": 92.7921,
      "step": 7950
    },
    {
      "epoch": 7.8125,
      "grad_norm": 0.023511216044425964,
      "learning_rate": 4.3750000000000005e-06,
      "loss": 92.8793,
      "step": 8000
    },
    {
      "epoch": 7.861328125,
      "grad_norm": 0.02495458535850048,
      "learning_rate": 4.27734375e-06,
      "loss": 93.0355,
      "step": 8050
    },
    {
      "epoch": 7.91015625,
      "grad_norm": 0.028587227687239647,
      "learning_rate": 4.1796875e-06,
      "loss": 93.2758,
      "step": 8100
    },
    {
      "epoch": 7.958984375,
      "grad_norm": 0.03073923848569393,
      "learning_rate": 4.0820312500000005e-06,
      "loss": 93.3474,
      "step": 8150
    },
    {
      "epoch": 8.0078125,
      "grad_norm": 0.024481715634465218,
      "learning_rate": 3.984375e-06,
      "loss": 93.5067,
      "step": 8200
    },
    {
      "epoch": 8.056640625,
      "grad_norm": 0.024879347532987595,
      "learning_rate": 3.88671875e-06,
      "loss": 93.6728,
      "step": 8250
    },
    {
      "epoch": 8.10546875,
      "grad_norm": 0.025162599980831146,
      "learning_rate": 3.7890625e-06,
      "loss": 93.8444,
      "step": 8300
    },
    {
      "epoch": 8.154296875,
      "grad_norm": 0.02785075642168522,
      "learning_rate": 3.6914062500000004e-06,
      "loss": 93.8964,
      "step": 8350
    },
    {
      "epoch": 8.203125,
      "grad_norm": 0.02368171326816082,
      "learning_rate": 3.59375e-06,
      "loss": 94.1032,
      "step": 8400
    },
    {
      "epoch": 8.251953125,
      "grad_norm": 0.026922957971692085,
      "learning_rate": 3.4960937500000003e-06,
      "loss": 94.1643,
      "step": 8450
    },
    {
      "epoch": 8.30078125,
      "grad_norm": 0.024728311225771904,
      "learning_rate": 3.3984375000000004e-06,
      "loss": 94.3531,
      "step": 8500
    },
    {
      "epoch": 8.349609375,
      "grad_norm": 0.03311720862984657,
      "learning_rate": 3.30078125e-06,
      "loss": 94.4757,
      "step": 8550
    },
    {
      "epoch": 8.3984375,
      "grad_norm": 0.02411452680826187,
      "learning_rate": 3.2031250000000004e-06,
      "loss": 94.5611,
      "step": 8600
    },
    {
      "epoch": 8.447265625,
      "grad_norm": 0.02331574261188507,
      "learning_rate": 3.10546875e-06,
      "loss": 94.6312,
      "step": 8650
    },
    {
      "epoch": 8.49609375,
      "grad_norm": 0.02646639756858349,
      "learning_rate": 3.0078125000000003e-06,
      "loss": 94.7054,
      "step": 8700
    },
    {
      "epoch": 8.544921875,
      "grad_norm": 0.02765042707324028,
      "learning_rate": 2.9101562500000004e-06,
      "loss": 94.7449,
      "step": 8750
    },
    {
      "epoch": 8.59375,
      "grad_norm": 0.028508083894848824,
      "learning_rate": 2.8125e-06,
      "loss": 94.9008,
      "step": 8800
    },
    {
      "epoch": 8.642578125,
      "grad_norm": 0.02292552776634693,
      "learning_rate": 2.7148437500000003e-06,
      "loss": 95.087,
      "step": 8850
    },
    {
      "epoch": 8.69140625,
      "grad_norm": 0.022494258359074593,
      "learning_rate": 2.6171875e-06,
      "loss": 95.1544,
      "step": 8900
    },
    {
      "epoch": 8.740234375,
      "grad_norm": 0.02762797474861145,
      "learning_rate": 2.5195312500000003e-06,
      "loss": 95.2857,
      "step": 8950
    },
    {
      "epoch": 8.7890625,
      "grad_norm": 0.023197107017040253,
      "learning_rate": 2.421875e-06,
      "loss": 95.2772,
      "step": 9000
    },
    {
      "epoch": 8.837890625,
      "grad_norm": 0.023137614130973816,
      "learning_rate": 2.32421875e-06,
      "loss": 95.4075,
      "step": 9050
    },
    {
      "epoch": 8.88671875,
      "grad_norm": 0.02175832912325859,
      "learning_rate": 2.2265625000000003e-06,
      "loss": 95.4852,
      "step": 9100
    },
    {
      "epoch": 8.935546875,
      "grad_norm": 0.023311801254749298,
      "learning_rate": 2.12890625e-06,
      "loss": 95.5378,
      "step": 9150
    },
    {
      "epoch": 8.984375,
      "grad_norm": 0.02334410883486271,
      "learning_rate": 2.0312500000000002e-06,
      "loss": 95.6655,
      "step": 9200
    },
    {
      "epoch": 9.033203125,
      "grad_norm": 0.025766218081116676,
      "learning_rate": 1.93359375e-06,
      "loss": 95.7085,
      "step": 9250
    },
    {
      "epoch": 9.08203125,
      "grad_norm": 0.02385449968278408,
      "learning_rate": 1.8359375000000002e-06,
      "loss": 95.8461,
      "step": 9300
    },
    {
      "epoch": 9.130859375,
      "grad_norm": 0.025387009605765343,
      "learning_rate": 1.7382812500000001e-06,
      "loss": 95.8509,
      "step": 9350
    },
    {
      "epoch": 9.1796875,
      "grad_norm": 0.023549316450953484,
      "learning_rate": 1.640625e-06,
      "loss": 95.9514,
      "step": 9400
    },
    {
      "epoch": 9.228515625,
      "grad_norm": 0.026765627786517143,
      "learning_rate": 1.54296875e-06,
      "loss": 95.9884,
      "step": 9450
    },
    {
      "epoch": 9.27734375,
      "grad_norm": 0.02408648654818535,
      "learning_rate": 1.4453125000000002e-06,
      "loss": 95.9876,
      "step": 9500
    },
    {
      "epoch": 9.326171875,
      "grad_norm": 0.024387609213590622,
      "learning_rate": 1.3476562500000001e-06,
      "loss": 96.0684,
      "step": 9550
    },
    {
      "epoch": 9.375,
      "grad_norm": 0.02580801025032997,
      "learning_rate": 1.25e-06,
      "loss": 96.0573,
      "step": 9600
    },
    {
      "epoch": 9.423828125,
      "grad_norm": 0.023554978892207146,
      "learning_rate": 1.15234375e-06,
      "loss": 96.2312,
      "step": 9650
    },
    {
      "epoch": 9.47265625,
      "grad_norm": 0.028695903718471527,
      "learning_rate": 1.0546875e-06,
      "loss": 96.2095,
      "step": 9700
    },
    {
      "epoch": 9.521484375,
      "grad_norm": 0.023069025948643684,
      "learning_rate": 9.570312500000002e-07,
      "loss": 96.2252,
      "step": 9750
    },
    {
      "epoch": 9.5703125,
      "grad_norm": 0.02536209113895893,
      "learning_rate": 8.59375e-07,
      "loss": 96.2945,
      "step": 9800
    },
    {
      "epoch": 9.619140625,
      "grad_norm": 0.024078303948044777,
      "learning_rate": 7.617187500000001e-07,
      "loss": 96.202,
      "step": 9850
    },
    {
      "epoch": 9.66796875,
      "grad_norm": 0.0230227280408144,
      "learning_rate": 6.640625e-07,
      "loss": 96.3158,
      "step": 9900
    },
    {
      "epoch": 9.716796875,
      "grad_norm": 0.023371057584881783,
      "learning_rate": 5.6640625e-07,
      "loss": 96.3542,
      "step": 9950
    },
    {
      "epoch": 9.765625,
      "grad_norm": 0.021163586527109146,
      "learning_rate": 4.6875000000000006e-07,
      "loss": 96.3139,
      "step": 10000
    }
  ],
  "logging_steps": 50,
  "max_steps": 10240,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
